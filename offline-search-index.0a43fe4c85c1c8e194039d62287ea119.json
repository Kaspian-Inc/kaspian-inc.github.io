














[{"body":"Amazon Web Services (AWS) Kaspian relies on an existing AWS account where it will host the compute and some elements of data storage. An IAM role for Kaspian services to assume should be created to facilitate this. The IAM Role ARN and AWS region are required for Kaspian to function.\n  The role ARN should have the following policies to access services from your account:\n AmazonEC2ReadOnlyAccess A custom policy allowing AWS STS AssumeRole permissions. The following JSON policy template can be used  { \"Version\": \"2012-10-17\", \"Statement\": { \"Effect\": \"Allow\", \"Action\": \"sts:AssumeRole\", \"Resource\": \"arn:aws:iam::\u003cAmazon account id\u003e:role/*\" } }  A custom policy allowing access to your staging bucket. The following JSON policy template can be used  { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": [ \"s3:*\" ], \"Resource\": [ \"arn:aws:s3:::\u003cS3 staging bucket\u003e\", \"arn:aws:s3:::\u003cS3 staging bucket\u003e/*\" ] } ] }  In addition, this role requires a few cross-account access policies to have access to Kaspian-specific resources. Please contact us for instructions on setting this up.  Support for Google Cloud Platform (GCP) and Microsoft Azure is coming soon.\nGithub Code storage for Kaspian pipelines is primarily handled via Github. In order to access the Github repo, Kaspian needs a Github personal access token (PAT) with read permissions for the account. This is a required piece of information.\n  Instructions to create a personal access token can be found here. Only repo permissions need to be provided.\nSlack An optional integration is support for Slack channel notifications of pipeline failures. Provide an access token for the desired Slack workspace to enable the ability to receive Slack messages when scheduled pipelines encounter errors.\n  To receive notifications over Slack refer to these instructions to make a custom Slack App. This can be called whatever you wish. When configuring the scope for the app, at a minimum, you will need to provide channels:join, channels:read, and chat:write permissions as the Bot Token Scope. Once the App is created and linked to your Slack workspace, you can pick specific channels to give it access. Then, simply copy the access token from the App and provide it to Kaspian. Kaspian will automatically be able to detect which channels you the App has access to. You can configure which one receives the notification when setting up a pipeline.\n","categories":"","description":"Kaspian supports integrations with several external services, including AWS, Github, and Slack.\n","excerpt":"Kaspian supports integrations with several external services, …","ref":"/gettingstarted/addingintegrations/","tags":"","title":"Adding Integrations"},{"body":"From the Task and Schema view, you can enter the pipeline builder view by pressing the Graph button as shown.\n  Assembling the pipeline Pipelines are assembled via the Pipelines tab. Here, you can organize Tasks as nodes on a DAG.\n  The available bank of tasks for a given pipeline are shown in the list on the bottom right. Click Add next to each Task to add it to the Pipeline Builder. Click and drag to move the tasks around. Tasks can be connected together by pressing Shift+Enter while dragging from one Task to another. The Pipeline Builder will perform checks to ensure that any connection made is valid, e.g., the schemas match, no cycles, etc.\nVersioning Pipeline snapshots are static, permanently fixed versions of a given pipeline. Press the Snapshot button above the Pipeline Builder to take a snapshot. These snapshots can then be deployed at the specified schedule from the Overview tab.\n","categories":"","description":"Pipelines can be constructed via a simple UI.\n","excerpt":"Pipelines can be constructed via a simple UI.\n","ref":"/pipelinemanagement/construction/","tags":"","title":"Construction"},{"body":"","categories":"","description":"Initial configuration steps to get started with Kaspian.\n","excerpt":"Initial configuration steps to get started with Kaspian.\n","ref":"/gettingstarted/","tags":"","title":"Getting Started"},{"body":"Schemas define the structure of data at any point within a Pipeline graph. Because they are required at every transformation within a graph, schemas ensure that Kaspian Pipelines are strongly typed and thereby protect the data integrity of downstream data sinks.\n  The Schema editor can be accessed via the Pipelines tab: after navigating to Pipelines, the Components view and the Graph Builder view can be toggled using the top right button.\n  Like Tasks, Schemas can be scoped to specific Pipelines or be configured to have global scope (i.e. used in any Pipeline). Scoping serves both to help organize Schemas and to facilitate their reuse in different Pipelines. Note that while the scope of a Schema cannot be changed after it has been created, Schemas can be cloned into other Pipelines scopes or into the global scope using the Schema editor.\nThe editor also enables Schema to be edited and deleted. Schemas cannot be deleted until all Tasks, Pipelines, and Datastores that use them are also deleted and Kaspian will present the user with the relevant dependency conflict chain if a delete operation is requested.\nTask Schemas All Tasks have either an input Schema or an output Schema; many require both. The input Schema defines the structure of data entering a Task while the output Schema defines the structure of data exiting a Task.\n  A Schema consists of an ordered array of fields. A field has four elements: name, description, datatype, and nullable. name refers to the column name being referenced. Note that this value must be unique within a given Schema. description is meant to provide a space for users to add any useful documentation about the field. datatype values must be selected from the following list of supported options:\n   Datatype Description     ARRAY Array-type values   BINARY Binary values   BOOLEAN True or false values   BYTE Byte values   DATE Dates without time values   DECIMAL Rational values with specified precision   DOUBLE Double precision values   FLOAT Floating point values   INTEGER Integer values   LONG 32-bit signed integer values   MAP Key-value pairs   SHORT 16-bit signed integer values   STRING Text or varchar values   TIMESTAMP Dates with time(zone) values    In general, Kaspian is compatible with any datatype supported by Apache Spark and maps types from Datastores to this list the same way a Spark engine would.\nThe nullable flag is a boolean option that specifies if the value for that specific field is allowed to be null. This option can serve as a valuable data integrity check for required fields.\nDatastore Schemas Tables registered in flat file/data lake environments such as AWS S3 can be added as Datastores. This abstraction allows these resources to behave identically to SQL Datastores such as Snowflake and Postgres within the Kaspian compute layer. Kaspian requires that these Datastores have a Schema attached so that data integrity can be programmatically enforced. It is recommended that Datastore Schemas have global scope so that they can be reused by other Pipelines.\n","categories":"","description":"Flexible data structures\n","excerpt":"Flexible data structures\n","ref":"/pipelinecomponents/schemas/","tags":"","title":"Schemas"},{"body":"Running SQL queries Kaspian provides a fully-featured query console for users to run SQL statements on their connected SQL databases.\n  The query console comes with 3 main components: a workspace for writing SQL queries, a database inspector, and a query preview table.\nWorkspace Switching to the Workspace view changes the left-hand panel to the following.\n  Kaspian provides two folders for saving your SQL worksheets. The Personal folder includes all SQL scripts that only the given user can view. The Shared folder allows SQL scripts to be shared across your organization. Files can be dragged between these folders to change their visibility. Currently Kaspian only supports saving SQL files, but support is coming for other file-types and for allowing file uploads.\nDatastore Tree Switching to the Datastores view will bring up a hierarchical view of a given SQL database.\n  By expanding the tree, Kaspian provides metadata about the structure of your database down to the individual column names and types.\nQuery preview At the bottom of the console, when a SELECT statement has finished executing, a preview of the resulting data is shown. This data can be downloaded as a CSV and is automatically limited to the first 1,000,000 rows of the query.\n","categories":"","description":"The query console allows you to create and share worksheets for running SQL on datastores.\n","excerpt":"The query console allows you to create and share worksheets for …","ref":"/miscellaneous/console/","tags":"","title":"SQL Query Console"},{"body":"Postgres, Aurora, and Redshift Kaspian supports data ingest from most Postgres-compatible data stores. Simply select the Postgres engine and fill out the form that appears.\n   Datastore Name: Name by which you will refer to this SQL datastore Host: Host name or IP address of the SQL datastore Port: Port number of the SQL datastore User: Username used to connect to the SQL datastore Password: Password for the user to connect to the SQL datastore Database: Database this connection will used for in the SQL datastore  Support for SSH Tunneling via Bastion host For SQL databases, SSH tunneling is also supported through Bastion host servers. To configure this, simply provide enable Use Bastion Host and provide the corresponding connection details. You must upload a .pem file with the private key to connect to the Bastion host.\n   Bastion User: Username used to SSH into the Bastion host Bastion Host: Host name or IP address of the Bastion host Bastion Port: Port number of the Bastion host Private Key: .pem file containing the private key used to SSH into the Bastion host  Note, SSH tunneling to a SQL datastore will add additional latency to the SQL statements that are run\nSnowflake Kaspian data pipelines also support data ingest via SQl from Snowflake data stores.\n   Datastore Name: Name by which you will refer to this Snowflake datastore Account: Snowflake full account URL, e.g., xy12345.us-east-1.snowflakecomputing.com User: Username used to connect to the Snowflake datastore Password: Password for the user to connect to the Snowflake datastore Warehouse: Specifies the virtual warehouse to use once connected. The specified warehouse should be an existing warehouse for which the specified default role has privileges. Role: Specifies the default access control role to use in the Snowflake session. Ensure this role is assigned to the user Database: Specifies the default database to use once connected  Kaspian currently does not support SSH tunneling to Snowflake datastores.\nAWS S3 Data can also be ingested directly from Parquet or CSV files on AWS S3.\n   Name: Name by which you will refer to this S3 datastore Path: S3 path this datastore is located at, e.g., s3://\u003cbucket\u003e/\u003cpath\u003e/\u003cto\u003e/\u003cfile or folder\u003e Data Format: Format of the data located at the provided S3 path. Currently, Parquet and CSV are supported. If CSV, an additional dropdown will appear to indicate whether the CSV file contains a header row. Schema: Dropdown to indicate the Kaspian schema that defines the data stored at the S3 path AWS Integration: Dropdown to select AWS Role ARN used to access this S3 datastore (currently only 1 ARN role is supported at a time per account)  ","categories":"","description":"Kaspian supports popoular SQL databases and AWS S3 as datastores\n","excerpt":"Kaspian supports popoular SQL databases and AWS S3 as datastores\n","ref":"/gettingstarted/addingdatastores/","tags":"","title":"Adding Datastores"},{"body":"Setting a Schedule Schedules can be configured via a number of pre-defined intervals or a custom one defined by a user-provided 5-character cron string. This is compatible with the standard cron spec and can be generated via cron string calculators available online.\n  Configuring Notifications Notifications can be configured to trigger on any pipeline exeuction failure. Kaspian offers support for Slack, email, and text notifications. These can be selected via the Notifications dropdown.\n  Pipeline Actions There are 2 main modes for a pipeline: an Activated state and a Deactivated state. When a pipeline is activated, it will run at the specified schedule. When a pipeline is inactive, it will not run. When activating a pipeline, a pop-up appears to select a specific pipeline snapshot. Only pipeline snapshots can be activated, so one must be created first.\n  In the activated state, a pipeline can be started ad-hoc at any point in time. In addition, a currently-running pipeline can be stopped at any time.\nPrevious pipeline executions are depicted as a series of colored dots within the same row as each active Pipeline. The colors correspond to the following statuses:\n Blue: currently executing Red: failed Green: succeeded Gray: cancelled  Each dot can be clicked to visit the Inspector view of that specific execution.\n","categories":"","description":"Pipelines are easily deployed in a few clicks.\n","excerpt":"Pipelines are easily deployed in a few clicks.\n","ref":"/pipelinemanagement/deployment/","tags":"","title":"Deployment"},{"body":"Pipelines are composed of Tasks, each of which is associated with one or more Schemas.\n","categories":"","description":"Powerful abstractions that simplify pipeline development workflows\n","excerpt":"Powerful abstractions that simplify pipeline development workflows\n","ref":"/pipelinecomponents/","tags":"","title":"Pipeline Components"},{"body":"Tasks are the principal units of work in a Pipeline graph. They enable data to be read in, transformed, and written back out to connected Datastores and thereby greatly simplify data manipulation in any environment at any scale.\nThe Task editor can be accessed via the Pipelines tab: after navigating to Pipelines, the Components view and the Graph Builder view can be toggled using the top right button.\n  Like Schemas, Tasks can be scoped to specific Pipelines or be configured to have global scope (i.e. used in any Pipeline). Scoping serves both to help organize Tasks and to facilitate their reuse in different Pipelines. Note that while the scope of a Task cannot be changed after it has been created, Tasks can be cloned into other Pipeline scopes or into the global scope using the Task editor.\nThe editor also enables Tasks to be edited and deleted. Tasks cannot be deleted until all Pipeline that use them are also deleted and Kaspian will present the user with the relevant dependency conflict chain if a delete operation is requested.\nAll Tasks require name, type, and description fields and the name field is required to be unique for Tasks of a given type within a given Pipeline.\nData Readers Data Readers are used to read data from registered Datastores. As they are the only task with this capability, Data Readers serve as the root nodes for all Pipeline graphs. In a Pipeline graph, Data Readers may only have outbound edges.\n  Additionally, Data Readers have the following fields:\nSource Datastore: The datastore to read from.\nCode Source: A link to a .sql file on GitHub. The SQL query must be Spark SQL compliant. The file can be on any branch in any repository so long as the connected Github integration has read access to it. The SQL file contains the query to be run against the source Datastore in order to load in data.\n1 2 3 4 5 6 7 8  SELECT start_station_id ss_id, start_station_latitude ss_lat, start_station_longitude ss_lon, end_station_id es_id, end_station_latitude es_lat, end_station_longitude es_lon FROM sample.nyc_citibikes   For S3 datastores, note that the special constant data is used to indicate the table object. In the above example, sample.nyc_citibikes would be replaced with data if it was from S3 instead of a SQL database.\nOutput Schema: The schema of the data produced by the query contained in the code source.\nData Writers Data Writers are used to write data to registered Datastores. As they are the only task with this capability, Data Writers serve as the leaf (terminal) nodes for all Pipeline graphs. In a Pipeline graph, Data Writers may only have inbound edges.\n  Additionally, Data Writers have the following fields:\nDestination Datastore: The Datastore to write to.\nInput Schema: The Schema of the data being written to the specified Datastore. For S3 Datastores this should exactly match the Schema specified for that Datastore on the Datastore page.\nWrite Mode: A directive specifying the write mode behavior of the task. Data Writers can either append to or overwrite the specified table (or for S3 Datastores, the contents of the associated S3 path).\nDestination Table: The Datastore table to write to. This field is only enabled for SQL Datastores. It is advisable to explicitly specify the full table path (schema.table).\nSQL Transforms SQL Transforms allow for the manipulation of data, either from outputs of other transformations or directly from Data Readers, using SQL. This source-agnostic nature of the Kaspian execution engine means that a single SQL query is capable of accessing and operating upon data from multiple Datastores and/or Pipeline branches. In a Pipeline graph, SQL Transforms may have any number of both inbound and outbound edges.\n  Additionally, SQL Transforms have the following fields:\nOutput Schema: The schema of the data produced by the query contained in the code source.\nInput Schemas: Each inbound edge to a SQL Transform represents a different data source, corresponding either to a Data Reader output or to an intermediate output of another transform Task. In order to distinguish between these inbound edges when writing the transformation query each edge must be configured with an alias and a schema.\n  While the same schema can be used multiple times within a SQL Transform if the data sources being read from have the same structure, aliases must be unique. An alias refers to the name that the input source table is referred to in the SQL query contained in the code source (see example below). This abstraction means that any intermediate data stage can be referred to and operated upon as if it were a materialized table even if this isn’t the case.\nCode Source: A link to a .sql file on GitHub. The file can be on any branch in any repository so long as the connected Github integration has read access to it. The SQL file contains the query to be run using the aliases provided in the Input Schemas as table names. The query must be Spark SQL compliant. Below is an example query that utilizes the aliases above:\n1 2 3 4 5 6  SELECT starbucks_stops.stop_id, stop_names.stop_name FROM starbucks_stops JOIN stop_names ON starbucks_stops.stop_id = stop_names.stop_id;   Python Function Transforms The Python Function Transform allows for Python user-defined functions to be applied to any data. This transform only allows for a single input, but allows multiple outputs still.\n  Python DataFrame Transforms Python DataFrame Transforms allow for direct manipulation of data with Pandas or Pyspark. Similar to the SQL transform, this allows any number of inbound and outbound edges. This Task takes a Python code source structured as follows:\n  1 2 3 4 5 6 7 8 9 10 11 12  import pandas as pd def main(input_data: dict) -\u003e pd.DataFrame: from example_package import some_function # Extract dataframes df_citibikes = input_data['df_citibikes'] df_starbucks = input_data['df_starbucks'] df_final = some_function(df_starbucks, df_citibikes) return df_final   Due to the distributed nature of our compute engine, certain imports must be specified within the main function instead of the top of the file. Some libraries may fail to import otherwise.\n  ML Transforms Coming soon…\nData Validators Coming soon…\n","categories":"","description":"Read, write, and transform\n","excerpt":"Read, write, and transform\n","ref":"/pipelinecomponents/tasks/","tags":"","title":"Tasks"},{"body":"Account permission levels There are 3 levels of user permissions:\n Root: The Root user is able to perform all actions in the account, including managing billing, managing other users. Admin: Admin users are able to perform most Root actions. The exceptions are that it is only allowed to view the invoices for billing and view account details for other users instead of having full management control. Developer: Developer users are able to perform all pipeline management actions but are unable to view billing or details about other users.  Managing your account Account management settings are available under the Account option in the top-right settings icon. Once your account has been made, you can change your name, change your password, and toggle two-factor authentication settings.\n  Two-Factor authentication To enable two-factor authentication, click the toggle when editing your account preferences. This will display a QR code that can be scanned using most time-based one-time password (TOTP) apps like Google Authenticator or Duo Mobile. Once scanned, you will be requested to enter the resulting 6-digit code from the app to verify that the two-factor authentication has been set up correctly. If two-factor authentication is already enabled, you can disable it by simply toggling again.\nManaging account users New users in your org can be added from this page for Root and Admin accounts. Root accounts can add Admin and Developer users, but Admin accounts can only add Developer users. By clicking the Add button, the following form appears.\n  Simply specify the requested info about the user, and they will be sent an email with their login credentials. They will be asked to change their password upon logging in.\nThe Root account is also able to reset the password for any given account. Simply click on the user who’s account password must be reset and they will receive an email with instructions to do so.\n","categories":"","description":"An overview of the user management console.\n","excerpt":"An overview of the user management console.\n","ref":"/miscellaneous/users/","tags":"","title":"User Management"},{"body":"Payment Methods Payment methods can be managed in the Billing tab. Simply add your credit card number and address.\n  If you have multiple payment methods stored, the one that you want to use as default can be toggled here. Only one method can be default at a time.\nInvoices Clicking on the Invoice button will bring up the current and historical invoices.\n  The current invoice will be marked as “Draft” whereas paid ones will be marked “Paid.” Kaspian offers metered billing, so the itemized cost breakdown will be a proportion of the total AWS bill for the resources consumed running Kaspian infrastructure.\n","categories":"","description":"An overview of Kaspian's billing management console.\n","excerpt":"An overview of Kaspian's billing management console.\n","ref":"/miscellaneous/billing/","tags":"","title":"Billing"},{"body":"The Pipeline Inspector Execution logs There are two types of execution logs Kaspian displays for each Pipeline execution. There’s a pipeline execution log that presents any errors that resulted from starting up the pipeline. These are error logs produced by Kaspian if any errors occur while attempting to deploy the pipeline, e.g., the Gitub code path is invalid.\n  Once nodes in the Pipeline begin executing, they each have their own set of logs that can be displayed by clicking on the specific node and selecting the View Log button on the bottom righthand pane. These contain the Spark and Kubernetes error logs produced during execution of each node.\n  Query staging data When a given node is clicked, the intermediate data that resulted from it during a given execution can be queried via a SQL console. The data is located in the staging_data table. For example\n1 2 3  SELECT * FROM staging_data LIMIT 100;   This is a placeholder page. Replace it with your own content.\n Text can be **bold**, _italic_, or ~~strikethrough~~. [Links](https://gohugo.io) should be blue with no underlines (unless hovered over). There should be whitespace between paragraphs. Vape migas chillwave sriracha poutine try-hard distillery. Tattooed shabby chic small batch, pabst art party heirloom letterpress air plant pop-up. Sustainable chia skateboard art party banjo cardigan normcore affogato vexillologist quinoa meggings man bun master cleanse shoreditch readymade. Yuccie prism four dollar toast tbh cardigan iPhone, tumblr listicle live-edge VHS. Pug lyft normcore hot chicken biodiesel, actually keffiyeh thundercats photo booth pour-over twee fam food truck microdosing banh mi. Vice activated charcoal raclette unicorn live-edge post-ironic. Heirloom vexillologist coloring book, beard deep v letterpress echo park humblebrag tilde. 90's four loko seitan photo booth gochujang freegan tumeric listicle fam ugh humblebrag. Bespoke leggings gastropub, biodiesel brunch pug fashion axe meh swag art party neutra deep v chia. Enamel pin fanny pack knausgaard tofu, artisan cronut hammock meditation occupy master cleanse chartreuse lumbersexual. Kombucha kogi viral truffaut synth distillery single-origin coffee ugh slow-carb marfa selfies. Pitchfork schlitz semiotics fanny pack, ugh artisan vegan vaporware hexagon. Polaroid fixie post-ironic venmo wolf ramps **kale chips**.  There should be no margin above this first sentence.   Blockquotes should be a lighter gray with a border along the left side in the secondary color.   There should be no margin below this final sentence. ## First Header 2 This is a normal paragraph following a header. Knausgaard kale chips snackwave microdosing cronut copper mug swag synth bitters letterpress glossier **craft beer**. Mumblecore bushwick authentic gochujang vegan chambray meditation jean shorts irony. Viral farm-to-table kale chips, pork belly palo santo distillery activated charcoal aesthetic jianbing air plant woke lomo VHS organic. Tattooed locavore succulents heirloom, small batch sriracha echo park DIY af. Shaman you probably haven't heard of them copper mug, crucifix green juice vape *single-origin coffee* brunch actually. Mustache etsy vexillologist raclette authentic fam. Tousled beard humblebrag asymmetrical. I love turkey, I love my job, I love my friends, I love Chardonnay! Deae legum paulatimque terra, non vos mutata tacet: dic. Vocant docuique me plumas fila quin afuerunt copia haec o neque. On big screens, paragraphs and headings should not take up the full container width, but we want tables, code blocks and similar to take the full width. Scenester tumeric pickled, authentic crucifix post-ironic fam freegan VHS pork belly 8-bit yuccie PBR\u0026B. **I love this life we live in**. ## Second Header 2  This is a blockquote following a header. Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong. ### Header 3 ``` This is a code block following a header. ``` Next level leggings before they sold out, PBR\u0026B church-key shaman echo park. Kale chips occupy godard whatever pop-up freegan pork belly selfies. Gastropub Belinda subway tile woke post-ironic seitan. Shabby chic man bun semiotics vape, chia messenger bag plaid cardigan. #### Header 4 * This is an unordered list following a header. * This is an unordered list following a header. * This is an unordered list following a header. ##### Header 5 1. This is an ordered list following a header. 2. This is an ordered list following a header. 3. This is an ordered list following a header. ###### Header 6 | What | Follows | |-----------|-----------------| | A table | A header | | A table | A header | | A table | A header | ---------------- There's a horizontal rule above and below this. ---------------- Here is an unordered list: * Liverpool F.C. * Chelsea F.C. * Manchester United F.C. And an ordered list: 1. Michael Brecker 2. Seamus Blake 3. Branford Marsalis And an unordered task list: - [x] Create a Hugo theme - [x] Add task lists to it - [ ] Take a vacation And a \"mixed\" task list: - [ ] Pack bags - ? - [ ] Travel! And a nested list: * Jackson 5 * Michael * Tito * Jackie * Marlon * Jermaine * TMNT * Leonardo * Michelangelo * Donatello * Raphael Definition lists can be used with Markdown syntax. Definition headers are bold. Name : Godzilla Born : 1952 Birthplace : Japan Color : Green ---------------- Tables should have bold headings and alternating shaded rows. | Artist | Album | Year | |-------------------|-----------------|------| | Michael Jackson | Thriller | 1982 | | Prince | Purple Rain | 1984 | | Beastie Boys | License to Ill | 1986 | If a table is too wide, it should scroll horizontally. | Artist | Album | Year | Label | Awards | Songs | |-------------------|-----------------|------|-------------|----------|-----------| | Michael Jackson | Thriller | 1982 | Epic Records | Grammy Award for Album of the Year, American Music Award for Favorite Pop/Rock Album, American Music Award for Favorite Soul/R\u0026B Album, Brit Award for Best Selling Album, Grammy Award for Best Engineered Album, Non-Classical | Wanna Be Startin' Somethin', Baby Be Mine, The Girl Is Mine, Thriller, Beat It, Billie Jean, Human Nature, P.Y.T. (Pretty Young Thing), The Lady in My Life | | Prince | Purple Rain | 1984 | Warner Brothers Records | Grammy Award for Best Score Soundtrack for Visual Media, American Music Award for Favorite Pop/Rock Album, American Music Award for Favorite Soul/R\u0026B Album, Brit Award for Best Soundtrack/Cast Recording, Grammy Award for Best Rock Performance by a Duo or Group with Vocal | Let's Go Crazy, Take Me With U, The Beautiful Ones, Computer Blue, Darling Nikki, When Doves Cry, I Would Die 4 U, Baby I'm a Star, Purple Rain | | Beastie Boys | License to Ill | 1986 | Mercury Records | noawardsbutthistablecelliswide | Rhymin \u0026 Stealin, The New Style, She's Crafty, Posse in Effect, Slow Ride, Girls, (You Gotta) Fight for Your Right, No Sleep Till Brooklyn, Paul Revere, Hold It Now, Hit It, Brass Monkey, Slow and Low, Time to Get Ill | ---------------- Code snippets like `var foo = \"bar\";` can be shown inline. Also, `this should vertically align` ~~`with this`~~ ~~and this~~. Code can also be shown in a block element. ``` foo := \"bar\"; bar := \"foo\"; ``` Code can also use syntax highlighting. ```go func main() { input := `var foo = \"bar\";` lexer := lexers.Get(\"javascript\") iterator, _ := lexer.Tokenise(nil, input) style := styles.Get(\"github\") formatter := html.New(html.WithLineNumbers()) var buff bytes.Buffer formatter.Format(\u0026buff, style, iterator) fmt.Println(buff.String()) } ``` ``` Long, single-line code blocks should not wrap. They should horizontally scroll if they are too long. This line should be long enough to demonstrate this. ``` Inline code inside table cells should still be distinguishable. | Language | Code | |-------------|--------------------| | Javascript | `var foo = \"bar\";` | | Ruby | `foo = \"bar\"{` | ---------------- Small images should be shown at their actual size. ![](https://upload.wikimedia.org/wikipedia/commons/thumb/9/9e/Picea_abies_shoot_with_buds%2C_Sogndal%2C_Norway.jpg/240px-Picea_abies_shoot_with_buds%2C_Sogndal%2C_Norway.jpg) Large images should always scale down and fit in the content container. ![](https://upload.wikimedia.org/wikipedia/commons/thumb/9/9e/Picea_abies_shoot_with_buds%2C_Sogndal%2C_Norway.jpg/1024px-Picea_abies_shoot_with_buds%2C_Sogndal%2C_Norway.jpg) _The photo above of the Spruce Picea abies shoot with foliage buds: Bjørn Erik Pedersen, CC-BY-SA._ ## Components ### Alerts This is an alert.  Note This is an alert with a title.  Note This is an alert with a title and Markdown.  This is a successful alert.  This is a warning.  Warning This is a warning with a title.  ## Another Heading Add some sections here to see how the ToC looks like. Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong. ### This Document Inguina genus: Anaphen post: lingua violente voce suae meus aetate diversi. Orbis unam nec flammaeque status deam Silenum erat et a ferrea. Excitus rigidum ait: vestro et Herculis convicia: nitidae deseruit coniuge Proteaque adiciam *eripitur*? Sitim noceat signa *probat quidem*. Sua longis *fugatis* quidem genae. ### Pixel Count Tilde photo booth wayfarers cliche lomo intelligentsia man braid kombucha vaporware farm-to-table mixtape portland. PBR\u0026B pickled cornhole ugh try-hard ethical subway tile. Fixie paleo intelligentsia pabst. Ennui waistcoat vinyl gochujang. Poutine salvia authentic affogato, chambray lumbersexual shabby chic. ### Contact Info Plaid hell of cred microdosing, succulents tilde pour-over. Offal shabby chic 3 wolf moon blue bottle raw denim normcore poutine pork belly. ### External Links Stumptown PBR\u0026B keytar plaid street art, forage XOXO pitchfork selvage affogato green juice listicle pickled everyday carry hashtag. Organic sustainable letterpress sartorial scenester intelligentsia swag bushwick. Put a bird on it stumptown neutra locavore. IPhone typewriter messenger bag narwhal. Ennui cold-pressed seitan flannel keytar, single-origin coffee adaptogen occupy yuccie williamsburg chillwave shoreditch forage waistcoat. ``` This is the final element on the page and there should be no margin below this. ``` -- ","categories":"","description":"Kaspian provides several mechanisms to gain greater insight into your pipelines.\n","excerpt":"Kaspian provides several mechanisms to gain greater insight into your …","ref":"/pipelinemanagement/introspection/","tags":"","title":"Introspection"},{"body":"","categories":"","description":"Pipelines are how Kaspian organizes your workflows.\n","excerpt":"Pipelines are how Kaspian organizes your workflows.\n","ref":"/pipelinemanagement/","tags":"","title":"Pipeline Management"},{"body":"","categories":"","description":"Additional management features of Kaspian.\n","excerpt":"Additional management features of Kaspian.\n","ref":"/miscellaneous/","tags":"","title":"Miscellaneous"},{"body":"","categories":"","description":"This site provides the user documentation for Kaspian. \n","excerpt":"This site provides the user documentation for Kaspian. \n","ref":"/","tags":"","title":"Documentation"}]